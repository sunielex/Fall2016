{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function for Logistic Regression \n",
    "#Author = Chandan Avadhut , Rohni Avadhut , Sunil Agrawal \n",
    "#CRS = short form for our team (Chandan Avadhut , Rohni Avadhut , Sunil Agrawal )\n",
    "\n",
    "#Import Python libaries \n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import preprocessing #To compare Our Model with sklearn Model \n",
    "from sklearn.linear_model import LogisticRegression #To compare Our Model with sklearn Model\n",
    "from sklearn.cross_validation import train_test_split #To compare Our Model with sklearn Model\n",
    "from numpy import loadtxt, where\n",
    "\n",
    "\n",
    "# -------------------------Dataset Handling --------------------------------- \n",
    "#Read the Input  CSV file and create a list \n",
    "dataset=[]\n",
    "with open(\"monks-1.train.csv\", 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        if not row:\n",
    "            continue\n",
    "        dataset.append(row)\n",
    "\n",
    "#Number of row in the dataset = lenght of the list of list \n",
    "#Number of column in each row = length of the first list of list \n",
    "#Number of the tetha = number of features as Logistic regression hyposthesis  function =hθ(x) = g(θ1x1+ θ3x2 + θ4x3 )\n",
    "\n",
    "print (\"Number of row on the dataset\" , len(dataset))\n",
    "print (\"Leght of the each row=\" , len(dataset[1]))\n",
    "print (\"Number of the theta = Number of Features = (Length of each row -1) \", len(dataset[1])-1  )\n",
    "print (\"Initializing the all the theta as 0\")\n",
    "initial_theta = []\n",
    "for i in range(1,len(dataset[1])):\n",
    "    initial_theta.append(0)\n",
    "#alpha is a parameter called the learning rate . \n",
    "#Choosing a correct learning rate is very import We should chose learning rate as low as possible .\n",
    "#with having greater value of learning rate such as  2. Each iteration would take us farther away from the minimum! .\n",
    "#The only concern with using too small of a learning rate is that you will need to run more iterations of \n",
    "#gradient descent, increasing your training time.\n",
    "alpha = 0.1\n",
    "#iterations - number of loops to Convergence / Stopping Gradient Descent  .With higher value result will be more accurate but \n",
    "#it will increase process time with having high number \n",
    "iterations = 1000\n",
    "print (\"theta, alpha(learning rate),iterations are initialized and values are  = \" , \"initial_theta=\" , initial_theta , \"alpha=\" , alpha, \"iterations=\",iterations)    \n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "print(\"Creating separate array for Features and Predictions\")\n",
    "x=[]\n",
    "X=[]\n",
    "y=[]\n",
    "Y=[]\n",
    "#for i in range(1,10):\n",
    "for i in range(len(dataset)):\n",
    "    x=dataset[i][:len(dataset[1])-1]\n",
    "    y=dataset[i][len(dataset[1])-1:]\n",
    "    X.append(x.copy())\n",
    "    Y.append(y.copy())\n",
    "X = [[float(j) for j in i] for i in X]\n",
    "Y = [[float(j) for j in i] for i in Y]\n",
    "    \n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "X = np.array(X)\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "Y=np.array(Y)\n",
    "print(\"Separate aaray created for features and Predictions\")\n",
    "print(\"---------------------------------------------------------------------------------------------------\")\n",
    "print(\"Creating Training and Test Dataset with split 0.33 , If you want to change the split ratio then please update the split variable\")\n",
    "\n",
    "#Create Test and Train Set . As recommended initializing splitsize as .33 .    \n",
    "splitsize=0.33\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=splitsize)\n",
    "print(\"Train and Test Set Created . Lenght of the Test Set = \" , len(X_test) , \"Length of the training set =\" , len(X_train))\n",
    "print(\"---------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "#-------- Functions Specific for Logistic Regression -------------------------------------------\n",
    "\n",
    "#CRS_Sigmoid Compute sigmoid functoon using Formula => transformed = 1 / (1 + e^-x)\n",
    "#CRS_Sigmoid(z) computes the sigmoid of z.\n",
    "\n",
    "def CRS_Sigmoid(z):\n",
    "    GradientofZ = float(1.0 / float((1.0 + math.exp(-1.0*z))))\n",
    "    return GradientofZ \n",
    "\n",
    "\n",
    "#Hypothesis Function CRS_Hypothesis(theta, x) to return hypothesis value of Feature intances\n",
    "#hθ(x) = g(θ1x1+ θ3x2 + θ4x3 )\n",
    "\n",
    "def CRS_Hypothesis(theta, x):\n",
    "    z = 0\n",
    "    for i in range(len(theta)):\n",
    "        z += x[i]*theta[i]\n",
    "    return CRS_Sigmoid(z)\n",
    "\n",
    "#Cost Derivative Function -- This function will give us value for Cost Devrivative which will be used for Gradient Descent function\n",
    "#CostFunction_Derivative= 1/m(∑i=1,m[hθ(x)−y(i)]x(i)j)  = \n",
    "\n",
    "def CRS_CostFunction_Derivative(X,Y,theta,j,m,alpha):\n",
    "    sumofErrors = 0\n",
    "    for i in range(m):\n",
    "        xi = X[i]\n",
    "        xij = xi[j]\n",
    "        hypoi = CRS_Hypothesis(theta,X[i])\n",
    "        error = (hypoi - Y[i])*xij\n",
    "        sumofErrors += error\n",
    "    m = len(Y) \n",
    "    constant = float(alpha)/float(m)\n",
    "    Cost_DerivativeOfXY = constant * sumofErrors\n",
    "    return Cost_DerivativeOfXY\n",
    "\n",
    "\n",
    "#Final Gradient Descent function to calculate theta  \n",
    "#theta = theta - (alpha/m) * X' * (1./(1+exp(-X*theta)) - y) = theta - (alpha/m) * \n",
    "\n",
    "def CSR_GradientDescent_Function(X,Y,theta,m,alpha):\n",
    "    theta_new = []\n",
    "    for j in range(len(theta)):\n",
    "        CostGradientDescent_Derivative = CRS_CostFunction_Derivative(X,Y,theta,j,m,alpha)\n",
    "        theta_value = theta[j] - CostGradientDescent_Derivative\n",
    "        theta_new.append(theta_value)\n",
    "    return theta_new\n",
    "\n",
    "#Final Logistic Regression to Calculate theta and Prediction \n",
    "# Logistic Regression to Iterate 10000 to ConvergenceGradient Descent closely and Calculate final tetha to calculate Hypothesis \n",
    "#Once we have hypothetis calculate with the final tetha value ,We can predict the test dataset \n",
    "#If hypothetis > .5 = Then prediction is True (1)\n",
    "#If hypothetis < .5 = Then prediction is True (0)\n",
    "\n",
    "def CRS_LogisticRegression(X,Y,alpha,theta,num_iters):\n",
    "    m = len(Y)\n",
    "    for x in range(num_iters):\n",
    "        theta_GD = CSR_GradientDescent_Function(X,Y,theta,m,alpha)\n",
    "        theta = theta_GD\n",
    "        \n",
    "#Calculate Prediction and Accuracy on Test Set \n",
    "    counter=0\n",
    "    length = len(X_test)\n",
    "    for i in range(length):        \n",
    "        CRS_prediction = round(CRS_Hypothesis(X_test[i],theta)) #round function just round the float number to specified decimal here it will round everything to either 0 or 1 .e.g round(.47)=0 ,round(0.56)=1\n",
    "        ActualValue = Y_test[i]\n",
    "        if CRS_prediction == ActualValue:\n",
    "            counter += 1      \n",
    "    Accuracy = float(counter) / float(length)\n",
    "    print(\"Accuracy of Test Dataset with our Logistic regression\" , Accuracy)\n",
    "#Calculate Accuracy using sklearn to compare our Model \n",
    "    CRS = LogisticRegression()\n",
    "    Y_train_R=Y_train.ravel()\n",
    "    CRS.fit(X_train,Y_train_R)\n",
    "    print (\"Accuracy of Training Dataset with sklearn =\", CRS.score(X_test,Y_test))\n",
    "\n",
    "#Calling Logistic Regression Function     \n",
    "CRS_LogisticRegression(X,Y,alpha,initial_theta,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
